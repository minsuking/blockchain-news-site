import os
import re
import textwrap
from datetime import datetime
from urllib.parse import urlparse
import requests
import feedparser
from bs4 import BeautifulSoup

# ===== ì„¤ì • =====
RSS_URL = "https://your-blog-domain.com/rss"  # ğŸ”¹ ë„¤ ë¸”ë¡œê·¸ RSS ì£¼ì†Œë¡œ ë³€ê²½
CONTENT_BASE = "content/news"
IMAGE_BASE = "static/images/news"
DEFAULT_CATEGORY = "rss-import"  # ì¹´í…Œê³ ë¦¬ ê¸°ë³¸ê°’
TIME_SUFFIX = "T09:00:00+09:00"  # í•œêµ­ ê¸°ì¤€ ê³ ì • ì‹œê°„
# =================


def slugify(text: str) -> str:
    text = text.strip().lower()
    text = re.sub(r"[^a-z0-9ê°€-í£\- ]", "", text)
    text = text.replace(" ", "-")
    return text[:60]


def ensure_dir(path: str):
    if not os.path.exists(path):
        os.makedirs(path, exist_ok=True)


def extract_image_url(entry) -> str | None:
    """
    RSS ì•ˆì—ì„œ ì´ë¯¸ì§€ í›„ë³´ë¥¼ ì°¾ëŠ” í•¨ìˆ˜.
    - <media:content>, <enclosure>, description ì•ˆì˜ <img>, content ì•ˆì˜ <img>
    """
    # 1) media:content / enclosure
    media_thumbnail = entry.get("media_thumbnail") or entry.get("media_content")
    if media_thumbnail:
        try:
            return media_thumbnail[0].get("url")
        except Exception:
            pass

    if "enclosures" in entry and entry.enclosures:
        for enc in entry.enclosures:
            url = enc.get("href") or enc.get("url")
            if url:
                return url

    # 2) description / summary ì† HTML ì•ˆì˜ <img>
    html_candidates = []
    if "description" in entry:
        html_candidates.append(entry.description)
    if "summary" in entry and entry.summary not in html_candidates:
        html_candidates.append(entry.summary)
    if "content" in entry:
        for c in entry.content:
            html_candidates.append(c.value)

    for html in html_candidates:
        soup = BeautifulSoup(html, "html.parser")
        img = soup.find("img")
        if img and img.get("src"):
            return img["src"]

    return None


def clean_html_to_markdown(html: str) -> str:
    """
    ìµœì†Œí•œë§Œ HTML íƒœê·¸ ì œê±°í•´ì„œ Hugo markdownìœ¼ë¡œ ì“¸ ìˆ˜ ìˆê²Œ ì •ë¦¬.
    (ì™„ì „í•œ markdown ë³€í™˜ì´ í•„ìš”í•œ ê²½ìš°ëŠ” ë‚˜ì¤‘ì— ë³„ë„ ì²˜ë¦¬)
    """
    soup = BeautifulSoup(html, "html.parser")

    # ì¤„ë°”ê¿ˆ íƒœê·¸ ì²˜ë¦¬
    for br in soup.find_all(["br", "hr"]):
        br.replace_with("\n")

    text = soup.get_text("\n")
    # ì—°ì† ê³µë°±/ì¤„ë°”ê¿ˆ ì •ë¦¬
    lines = [line.strip() for line in text.splitlines()]
    lines = [line for line in lines if line]
    return "\n\n".join(lines)


def main():
    print(f"[INFO] RSS ê°€ì ¸ì˜¤ëŠ” ì¤‘: {RSS_URL}")
    feed = feedparser.parse(RSS_URL)

    if feed.bozo:
        print("[ERROR] RSS íŒŒì‹±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. RSS URLì„ ë‹¤ì‹œ í™•ì¸í•˜ì„¸ìš”.")
        return

    print(f"[INFO] RSS ì œëª©: {feed.feed.get('title', 'ì œëª© ì—†ìŒ')}")
    print(f"[INFO] í•­ëª© ê°œìˆ˜: {len(feed.entries)}")

    for entry in feed.entries:
        # ì œëª©
        title = entry.get("title", "ì œëª© ì—†ìŒ").strip()
        # ë§í¬(ì¶œì²˜)
        link = entry.get("link", "").strip()

        # ë‚ ì§œ
        if "published_parsed" in entry and entry.published_parsed:
            dt = datetime(*entry.published_parsed[:6])
        elif "updated_parsed" in entry and entry.updated_parsed:
            dt = datetime(*entry.updated_parsed[:6])
        else:
            dt = datetime.now()

        date_str = dt.strftime("%Y-%m-%d")
        year = dt.strftime("%Y")
        month = dt.strftime("%m")

        # slug ìƒì„±
        slug_base = slugify(title) or "untitled"
        slug = f"{date_str}-{slug_base}"

        # í´ë” ê²½ë¡œ
        content_dir = os.path.join(CONTENT_BASE, year, month)
        ensure_dir(content_dir)
        md_path = os.path.join(content_dir, f"{slug}.md")

        if os.path.exists(md_path):
            print(f"[SKIP] ì´ë¯¸ ì¡´ì¬: {md_path}")
            continue

        # ë³¸ë¬¸ (content > summary > description ìˆœì„œë¡œ)
        body_html = ""
        if "content" in entry and entry.content:
            body_html = entry.content[0].value
        elif "summary" in entry:
            body_html = entry.summary
        elif "description" in entry:
            body_html = entry.description
        else:
            body_html = ""

        body_text = clean_html_to_markdown(body_html)

        # ëŒ€í‘œ ì´ë¯¸ì§€ URL
        img_url = extract_image_url(entry)
        featured_image = ""

        if img_url:
            try:
                parsed = urlparse(img_url)
                ext = os.path.splitext(parsed.path)[1]
                if ext.lower() not in [".jpg", ".jpeg", ".png", ".webp"]:
                    ext = ".jpg"
                img_dir = os.path.join(IMAGE_BASE, year, month)
                ensure_dir(img_dir)
                img_filename = slug + ext
                img_path = os.path.join(img_dir, img_filename)

                print(f"[IMG] ë‹¤ìš´ë¡œë“œ: {img_url} -> {img_path}")
                r = requests.get(img_url, timeout=10)
                if r.status_code == 200:
                    with open(img_path, "wb") as f:
                        f.write(r.content)
                    featured_image = f"/images/news/{year}/{month}/{img_filename}"
                else:
                    print(f"[WARN] ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ status={r.status_code}")
            except Exception as e:
                print(f"[WARN] ì´ë¯¸ì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

        # front matter ì‘ì„±
        front_matter = f"""---
title: "{title.replace('"', '\\"')}"
date: {date_str}{TIME_SUFFIX}
lastmod: {date_str}{TIME_SUFFIX}
draft: false
categories: ["{DEFAULT_CATEGORY}"]
tags: []
summary: ""
sourceUrl: "{link}"
"""

        if featured_image:
            front_matter += f'featuredImage: "{featured_image}"\n'

        front_matter += "---\n\n"

        full_content = front_matter + body_text + "\n"

        with open(md_path, "w", encoding="utf-8") as f:
            f.write(full_content)

        print(f"[OK] ìƒì„±: {md_path}")

    print("[DONE] RSS â†’ Hugo ë³€í™˜ ì™„ë£Œ")


if __name__ == "__main__":
    main()
